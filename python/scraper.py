"""
Web Scraping Tool with Firebase Integration
ãƒ¡ã‚¤ãƒ³ã®ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import requests
from bs4 import BeautifulSoup
import time
import logging
import argparse
from datetime import datetime
from typing import List, Dict, Optional, Any
# import os  # ä»Šå¾Œã®æ‹¡å¼µç”¨ã«æ®‹ã—ã¦ãŠã
from dotenv import load_dotenv

from firebase_config import FirebaseManager

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class WebScraper:
    """ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.firebase_manager = FirebaseManager()
        
    def scrape_example_site(self, url: str) -> Optional[Dict[str, Any]]:
        """
        ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ãƒˆã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        å®Ÿéš›ã®ä½¿ç”¨æ™‚ã¯å¯¾è±¡ã‚µã‚¤ãƒˆã«åˆã‚ã›ã¦ä¿®æ­£ã—ã¦ãã ã•ã„
        """
        try:
            logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {url}")
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ã‚µãƒ³ãƒ—ãƒ«: ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜ã‚’å–å¾—
            title = soup.find('title')
            title_text = title.get_text().strip() if title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
            
            # ãƒ¡ã‚¿èª¬æ˜ã‚’å–å¾—
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            description = ""
            if meta_desc:
                try:
                    description = str(meta_desc.get('content', ''))  # type: ignore
                except:
                    description = ""
            
            # è¦‹å‡ºã—ã‚’å–å¾—
            headings = [h.get_text().strip() for h in soup.find_all(['h1', 'h2', 'h3'])]
            
            data: Dict[str, Any] = {
                'url': url,
                'title': title_text,
                'description': str(description),
                'headings': headings[:10],  # æœ€åˆã®10å€‹ã®ã¿
                'scraped_at': datetime.now().isoformat(),
                'content_length': len(response.content)
            }
            
            logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {title_text}")
            return data
            
        except requests.RequestException as e:
            logger.error(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ {url}: {e}")
            return None
        except Exception as e:
            logger.error(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ {url}: {e}")
            return None
    
    def scrape_multiple_urls(self, urls: List[str], delay: float = 1.0) -> List[Dict[str, Any]]:
        """
        è¤‡æ•°URLã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        """
        results: List[Dict[str, Any]] = []
        
        for i, url in enumerate(urls):
            logger.info(f"é€²è¡ŒçŠ¶æ³: {i+1}/{len(urls)}")
            
            data = self.scrape_example_site(url)
            if data:
                results.append(data)
                
                # Firebaseã«ä¿å­˜
                try:
                    self.firebase_manager.save_scraped_data(data)  # type: ignore
                    logger.info(f"Firebaseã«ä¿å­˜å®Œäº†: {data['title']}")
                except Exception as e:
                    logger.error(f"Firebaseä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®ãŸã‚ã®å¾…æ©Ÿ
            if i < len(urls) - 1:
                time.sleep(delay)
        
        return results
    
    def run_scheduled_scraping(self) -> List[Dict[str, Any]]:
        """
        å®šæœŸå®Ÿè¡Œç”¨ã®ãƒ¡ã‚½ãƒƒãƒ‰
        """
        # ã‚µãƒ³ãƒ—ãƒ«URLï¼ˆå®Ÿéš›ã®ä½¿ç”¨æ™‚ã¯å¯¾è±¡URLãƒªã‚¹ãƒˆã«å¤‰æ›´ï¼‰
        sample_urls = [
            "https://example.com",
            "https://httpbin.org/html",
        ]
        
        logger.info("å®šæœŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
        results = self.scrape_multiple_urls(sample_urls)
        logger.info(f"å®šæœŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(results)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†")
        
        return results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description='ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«')
    parser.add_argument('--vrchat', action='store_true', help='VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°')
    parser.add_argument('--vrchat-url', type=str, help='VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰URLã‚’ç›´æ¥æŒ‡å®š')
    parser.add_argument('--vrchat-file', type=str, help='VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®š')
    parser.add_argument('--count', type=int, default=20, help='å–å¾—ã™ã‚‹ãƒ¯ãƒ¼ãƒ«ãƒ‰æ•°ï¼ˆVRChatç”¨ï¼‰')
    parser.add_argument('--featured', action='store_true', help='ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ¯ãƒ¼ãƒ«ãƒ‰ã®ã¿ï¼ˆVRChatç”¨ï¼‰')
    parser.add_argument('--keyword', type=str, help='æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆVRChatç”¨ï¼‰')
    parser.add_argument('--delay', type=float, default=2.0, help='ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰')
    
    args = parser.parse_args()
    
    if args.vrchat or args.vrchat_url or args.vrchat_file:
        # VRChatã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        try:
            from vrchat_scraper import VRChatWorldScraper
            vrchat_scraper = VRChatWorldScraper()
            
            if args.vrchat_url:
                print(f"VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰URL {args.vrchat_url} ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¾ã™...")
                world_info = vrchat_scraper.scrape_world_by_url(args.vrchat_url)  # type: ignore
                
                if world_info:
                    print("\n=== VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ± ===")
                    print(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰ID: {world_info['world_id']}")
                    print(f"ã‚¿ã‚¤ãƒˆãƒ«: {world_info['title']}")
                    print(f"åˆ¶ä½œè€…: {world_info['creator']}")
                    print(f"ã‚µãƒ ãƒã‚¤ãƒ«: {world_info['thumbnail_url']}")
                    print(f"èª¬æ˜: {world_info['description']}")
                    print(f"å®šå“¡: {world_info['capacity']}")
                    print(f"å…¬é–‹æ—¥: {world_info['published']}")
                    print(f"å–å¾—æ—¥æ™‚: {world_info['scraped_at']}")
                    
                    # Firebaseã«ä¿å­˜
                    try:
                        vrchat_scraper.firebase_manager.save_vrchat_world_data(world_info)  # type: ignore
                        print("\nFirebaseã«ä¿å­˜å®Œäº†ã—ã¾ã—ãŸ")
                    except Exception as e:
                        print(f"Firebaseä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                else:
                    print("ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    
            elif args.vrchat_file:
                print(f"VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« {args.vrchat_file} ã‹ã‚‰ãƒãƒƒãƒå‡¦ç†ã—ã¾ã™...")
                results = vrchat_scraper.scrape_worlds_from_file(args.vrchat_file)  # type: ignore
                
                if results:
                    print(f"\n=== ãƒãƒƒãƒå‡¦ç†çµæœ ===")
                    print(f"âœ… æˆåŠŸ: {len(results)}ä»¶ã®ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")  # type: ignore
                    
                    # çµæœã®æ¦‚è¦ã‚’è¡¨ç¤º
                    print("\nğŸ“Š å–å¾—ã—ãŸãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸€è¦§:")
                    for i, world in enumerate(results[:5], 1):  # type: ignore
                        print(f"{i}. {world.get('title', 'Unknown')} (ä½œè€…: {world.get('creator', 'Unknown')})")  # type: ignore
                    
                    if len(results) > 5:  # type: ignore
                        print(f"... ä»– {len(results) - 5} ä»¶")  # type: ignore
                    
                    # çµ±è¨ˆæƒ…å ±
                    total_capacity = sum(int(w.get('capacity', 0)) if str(w.get('capacity', '')).isdigit() else 0 for w in results)  # type: ignore
                    print(f"\nğŸ“ˆ çµ±è¨ˆæƒ…å ±:")
                    print(f"ãƒ»åˆè¨ˆå®šå“¡: {total_capacity}äºº")
                    print(f"ãƒ»å¹³å‡å®šå“¡: {total_capacity // len(results) if results else 0}äºº")  # type: ignore
                    
                else:
                    print("âŒ ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    
            else:
                print("VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
                
                if args.keyword:
                    print(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{args.keyword}' ã§æ¤œç´¢ä¸­...")
                    results = vrchat_scraper.search_worlds_by_keyword(args.keyword, args.count)  # type: ignore
                elif args.featured:
                    print("ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—ä¸­...")
                    results = vrchat_scraper.scrape_featured_worlds()  # type: ignore
                else:
                    print(f"äººæ°—ãƒ¯ãƒ¼ãƒ«ãƒ‰ {args.count}ä»¶ã‚’å–å¾—ä¸­...")
                    results = vrchat_scraper.scrape_popular_worlds(args.count)  # type: ignore
                
                print(f"\nçµæœ: {len(results)}ä»¶ã®VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¾ã—ãŸ")  # type: ignore
                for i, world in enumerate(results[:5], 1):  # type: ignore
                    print(f"{i}. {world['name']} (ä½œè€…: {world['author_name']}, äººæ°—: {world['popularity']}%)")  # type: ignore
                    
                if len(results) > 5:  # type: ignore
                    print(f"... ä»– {len(results) - 5} ä»¶")  # type: ignore
                
        except ImportError as e:
            print(f"VRChatã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        except Exception as e:
            print(f"VRChatã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            # ã‚¨ãƒ©ãƒ¼ã®æœ‰ç„¡ã«é–¢ã‚ã‚‰ãšã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            try:
                if 'vrchat_scraper' in locals():
                    vrchat_scraper.cleanup()  # type: ignore
            except:
                pass  # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
    
    else:
        # é€šå¸¸ã®Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        scraper = WebScraper()
        
        # ã‚µãƒ³ãƒ—ãƒ«URLï¼ˆå®Ÿéš›ã®ä½¿ç”¨æ™‚ã¯å¯¾è±¡URLãƒªã‚¹ãƒˆã«å¤‰æ›´ï¼‰
        sample_urls = [
            "https://example.com",
            "https://httpbin.org/html",
        ]
        
        print("ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã‚’é–‹å§‹ã—ã¾ã™...")
        results = scraper.scrape_multiple_urls(sample_urls)  # type: ignore
        
        print(f"\nçµæœ: {len(results)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¾ã—ãŸ")
        for result in results:
            print(f"- {result['title']} ({result['url']})")

if __name__ == "__main__":
    main()
