"""
VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
èªè¨¼ä»˜ãã§ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã‚’å–å¾—ã™ã‚‹
"""

import time
import logging
import json
import os
from typing import List, Dict, Optional, Any
from datetime import datetime
from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup
import re

from vrchat_auth import VRChatAuth
from firebase_config import FirebaseManager

logger = logging.getLogger(__name__)

class VRChatWorldScraper:
    """VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.auth = VRChatAuth()
        self.firebase_manager = FirebaseManager()
        self.base_url = "https://api.vrchat.cloud/api/1/"
        self.session_file = "config/vrchat_session.json"
        self.thumbnail_dir = "thumbnail"
        
        # ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(self.thumbnail_dir, exist_ok=True)
        
    def ensure_authenticated(self) -> bool:
        """èªè¨¼çŠ¶æ…‹ã‚’ç¢ºä¿"""
        # æ—¢å­˜ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è©¦è¡Œ
        if self.auth.load_session(self.session_file):
            logger.info("æ—¢å­˜ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨")
            return True
        
        # æ–°è¦ãƒ­ã‚°ã‚¤ãƒ³
        logger.info("æ–°è¦ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ã§ã™")
        if self.auth.login():
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
            self.auth.save_session(self.session_file)
            return True
        
        return False
    
    def download_thumbnail(self, thumbnail_url: str, world_id: str) -> Optional[str]:
        """
        ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜
        """
        if not thumbnail_url or thumbnail_url == '':
            logger.warning(f"ã‚µãƒ ãƒã‚¤ãƒ«URLãŒç©ºã§ã™: {world_id}")
            return None
            
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆãƒ¯ãƒ¼ãƒ«ãƒ‰ID + æ‹¡å¼µå­ï¼‰
            parsed_url = urlparse(thumbnail_url)
            file_extension = '.jpg'  # VRChatã‚µãƒ ãƒã‚¤ãƒ«ã¯é€šå¸¸jpg
            if '.' in parsed_url.path:
                file_extension = '.' + parsed_url.path.split('.')[-1]
            
            filename = f"{world_id}{file_extension}"
            filepath = os.path.join(self.thumbnail_dir, filename)
            
            # æ—¢ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if os.path.exists(filepath):
                logger.info(f"ã‚µãƒ ãƒã‚¤ãƒ«æ—¢å­˜: {filename}")
                return filepath
            
            # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            logger.info(f"ã‚µãƒ ãƒã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {thumbnail_url}")
            session = self.auth.get_authenticated_session()
            if not session:
                logger.error("èªè¨¼ã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒå–å¾—ã§ãã¾ã›ã‚“")
                return None
                
            response = session.get(thumbnail_url, timeout=30)
            response.raise_for_status()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            logger.info(f"âœ… ã‚µãƒ ãƒã‚¤ãƒ«ä¿å­˜å®Œäº†: {filename} ({len(response.content)} bytes)")
            return filepath
            
        except Exception as e:
            logger.error(f"âŒ ã‚µãƒ ãƒã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ {world_id}: {e}")
            return None
    
    def search_worlds(self, 
                     search: str = "",
                     featured: bool = False,
                     sort: str = "popularity",
                     user: str = "me",
                     number: int = 60,
                     offset: int = 0) -> List[Dict[str, Any]]:
        """
        ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚’æ¤œç´¢
        
        Args:
            search: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            featured: ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã•ã‚ŒãŸãƒ¯ãƒ¼ãƒ«ãƒ‰ã®ã¿
            sort: ã‚½ãƒ¼ãƒˆæ–¹æ³• (popularity, heat, trust, shuffle, random, favorites, 
                  reportScore, reportCount, publicationDate, labsPublicationDate, 
                  created, _created_at, updated, _updated_at, order, relevance)
            user: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            number: å–å¾—æ•° (æœ€å¤§100)
            offset: ã‚ªãƒ•ã‚»ãƒƒãƒˆ
        """
        if not self.ensure_authenticated():
            logger.error("èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return []
        
        try:
            session = self.auth.get_authenticated_session()
            if not session:
                return []
            
            # APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
            url = urljoin(self.base_url, "worlds")
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
            params: Dict[str, Any] = {
                'n': min(number, 100),  # æœ€å¤§100ã«åˆ¶é™
                'offset': offset,
                'sort': sort,
                'user': user
            }
            
            if search:
                params['search'] = search
            if featured:
                params['featured'] = 'true'
            
            logger.info(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰æ¤œç´¢: {params}")
            response = session.get(url, params=params)
            
            if response.status_code == 200:
                worlds_data = response.json()
                logger.info(f"{len(worlds_data)}ä»¶ã®ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—")
                return worlds_data
            else:
                logger.error(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text}")
                return []
                
        except Exception as e:
            logger.error(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰æ¤œç´¢ä¾‹å¤–: {e}")
            return []
    
    def get_world_details(self, world_id: str) -> Optional[Dict[str, Any]]:
        """
        ç‰¹å®šã®ãƒ¯ãƒ¼ãƒ«ãƒ‰ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
        """
        if not self.ensure_authenticated():
            return None
        
        try:
            session = self.auth.get_authenticated_session()
            if not session:
                return None
            
            url = urljoin(self.base_url, f"worlds/{world_id}")
            
            logger.info(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰è©³ç´°å–å¾—: {world_id}")
            response = session.get(url)
            
            if response.status_code == 200:
                world_data = response.json()
                logger.info(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰è©³ç´°å–å¾—æˆåŠŸ: {world_data.get('name', 'Unknown')}")
                return world_data
            else:
                logger.error(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰è©³ç´°å–å¾—ä¾‹å¤–: {e}")
            return None
    
    def get_world_instances(self, world_id: str) -> List[Dict[str, Any]]:
        """
        ãƒ¯ãƒ¼ãƒ«ãƒ‰ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æƒ…å ±ã‚’å–å¾—
        """
        if not self.ensure_authenticated():
            return []
        
        try:
            session = self.auth.get_authenticated_session()
            if not session:
                return []
            
            url = urljoin(self.base_url, f"worlds/{world_id}/instances")
            
            logger.info(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—: {world_id}")
            response = session.get(url)
            
            if response.status_code == 200:
                instances_data = response.json()
                logger.info(f"{len(instances_data)}å€‹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—")
                return instances_data
            else:
                logger.error(f"ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {response.status_code}")
                return []
                
        except Exception as e:
            logger.error(f"ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—ä¾‹å¤–: {e}")
            return []
    
    def process_world_data(self, world_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦Firebaseç”¨ã®å½¢å¼ã«å¤‰æ›
        """
        processed_data = {
            'world_id': world_data.get('id', ''),
            'name': world_data.get('name', ''),
            'description': world_data.get('description', ''),
            'author_name': world_data.get('authorName', ''),
            'author_id': world_data.get('authorId', ''),
            'capacity': world_data.get('capacity', 0),
            'recommended_capacity': world_data.get('recommendedCapacity', 0),
            'visits': world_data.get('visits', 0),
            'popularity': world_data.get('popularity', 0),
            'heat': world_data.get('heat', 0),
            'favorites': world_data.get('favorites', 0),
            'publication_date': world_data.get('publicationDate', ''),
            'labs_publication_date': world_data.get('labsPublicationDate', ''),
            'created_at': world_data.get('created_at', ''),
            'updated_at': world_data.get('updated_at', ''),
            'version': world_data.get('version', 0),
            'unity_version': world_data.get('unityVersion', ''),
            'release_status': world_data.get('releaseStatus', ''),
            'tags': world_data.get('tags', []),
            'image_url': world_data.get('imageUrl', ''),
            'thumbnail_image_url': world_data.get('thumbnailImageUrl', ''),
            'namespace': world_data.get('namespace', ''),
            'platform': world_data.get('platform', ''),
            'scraped_at': datetime.now().isoformat(),
            'instances': []  # å¾Œã§è¿½åŠ ã•ã‚Œã‚‹
        }
        
        return processed_data
    
    def scrape_popular_worlds(self, count: int = 100) -> List[Dict[str, Any]]:
        """
        äººæ°—ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        """
        logger.info(f"äººæ°—ãƒ¯ãƒ¼ãƒ«ãƒ‰ {count}ä»¶ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹")
        
        all_worlds = []
        processed_count = 0
        offset = 0
        batch_size = 60  # VRChat APIã®åˆ¶é™
        
        while processed_count < count:
            remaining = min(batch_size, count - processed_count)
            
            worlds = self.search_worlds(
                sort="popularity",
                number=remaining,
                offset=offset
            )
            
            if not worlds:
                logger.warning("ã“ã‚Œä»¥ä¸Šã®ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“")
                break
            
            for world in worlds:
                try:
                    # è©³ç´°æƒ…å ±ã‚’å–å¾—
                    detailed_world = self.get_world_details(world['id'])
                    if detailed_world:
                        processed_world = self.process_world_data(detailed_world)
                        
                        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æƒ…å ±ã‚‚å–å¾—
                        instances = self.get_world_instances(world['id'])
                        processed_world['instances'] = instances
                        
                        all_worlds.append(processed_world)
                        
                        # Firebaseã«ä¿å­˜
                        try:
                            self.firebase_manager.save_vrchat_world_data(processed_world)
                            logger.info(f"Firebaseä¿å­˜å®Œäº†: {processed_world['name']}")
                        except Exception as e:
                            logger.error(f"Firebaseä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                        
                        processed_count += 1
                        
                        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
                        time.sleep(0.5)  # 500mså¾…æ©Ÿ
                        
                    if processed_count >= count:
                        break
                        
                except Exception as e:
                    logger.error(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            offset += len(worlds)
            
            # æ¬¡ã®ãƒãƒƒãƒå‰ã«å°‘ã—å¾…æ©Ÿ
            time.sleep(1.0)
        
        logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(all_worlds)}ä»¶ã®ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†")
        return all_worlds
    
    def scrape_featured_worlds(self) -> List[Dict[str, Any]]:
        """
        ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã•ã‚ŒãŸãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        """
        logger.info("ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ¯ãƒ¼ãƒ«ãƒ‰ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹")
        
        worlds = self.search_worlds(featured=True, number=100)
        processed_worlds = []
        
        for world in worlds:
            try:
                detailed_world = self.get_world_details(world['id'])
                if detailed_world:
                    processed_world = self.process_world_data(detailed_world)
                    processed_worlds.append(processed_world)
                    
                    # Firebaseã«ä¿å­˜
                    try:
                        self.firebase_manager.save_vrchat_world_data(processed_world)
                        logger.info(f"Firebaseä¿å­˜å®Œäº†: {processed_world['name']}")
                    except Exception as e:
                        logger.error(f"Firebaseä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                    
                    time.sleep(0.5)
                    
            except Exception as e:
                logger.error(f"ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ¯ãƒ¼ãƒ«ãƒ‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        logger.info(f"ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(processed_worlds)}ä»¶")
        return processed_worlds
    
    def search_worlds_by_keyword(self, keyword: str, count: int = 50) -> List[Dict[str, Any]]:
        """
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚’æ¤œç´¢ã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        """
        logger.info(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{keyword}' ã§ãƒ¯ãƒ¼ãƒ«ãƒ‰æ¤œç´¢")
        
        worlds = self.search_worlds(search=keyword, number=count)
        processed_worlds = []
        
        for world in worlds:
            try:
                processed_world = self.process_world_data(world)
                processed_worlds.append(processed_world)
                
                # Firebaseã«ä¿å­˜
                try:
                    self.firebase_manager.save_vrchat_world_data(processed_world)
                    logger.info(f"Firebaseä¿å­˜å®Œäº†: {processed_world['name']}")
                except Exception as e:
                    logger.error(f"Firebaseä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãƒ¯ãƒ¼ãƒ«ãƒ‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        logger.info(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢å®Œäº†: {len(processed_worlds)}ä»¶")
        return processed_worlds
    
    def cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†"""
        if self.auth:
            self.auth.logout()

    def scrape_world_page(self, world_id: str) -> Optional[Dict[str, Any]]:
        """
        VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        """
        if not self.ensure_authenticated():
            return None
        
        try:
            session = self.auth.get_authenticated_session()
            if not session:
                return None
            
            # VRChatã®ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒšãƒ¼ã‚¸URL
            world_url = f"https://vrchat.com/home/world/{world_id}"
            
            logger.info(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: {world_url}")
            
            # Webãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—
            response = session.get(world_url)
            
            if response.status_code != 200:
                logger.error(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {response.status_code}")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã‚’æŠ½å‡º
            world_info = self._extract_world_info(soup, world_id)
            
            if world_info:
                logger.info(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±æŠ½å‡ºæˆåŠŸ: {world_info.get('title', 'Unknown')}")
                return world_info
            else:
                logger.warning("ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
                
        except Exception as e:
            logger.error(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¾‹å¤–: {e}")
            return None
    
    def _extract_world_info(self, soup: BeautifulSoup, world_id: str) -> Optional[Dict[str, Any]]:
        """
        HTMLã‹ã‚‰ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã‚’æŠ½å‡º
        """
        try:
            world_info = {
                'world_id': world_id,
                'title': '',
                'creator': '',
                'thumbnail_url': '',
                'description': '',
                'capacity': '',
                'published': '',
                'scraped_at': datetime.now().isoformat()
            }
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
            title_element = soup.find('h1') or soup.find('title')
            if title_element:
                world_info['title'] = title_element.get_text(strip=True)
            
            # åˆ¶ä½œè€…ã‚’æŠ½å‡º
            creator_element = soup.find('a', class_='user-link') or soup.find('span', class_='author')
            if creator_element:
                world_info['creator'] = creator_element.get_text(strip=True)
            
            # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒURLã‚’æŠ½å‡º
            thumbnail_element = soup.find('img', class_='world-image') or soup.find('img', {'alt': re.compile(r'world|thumbnail', re.I)})
            if thumbnail_element:
                world_info['thumbnail_url'] = thumbnail_element.get('src', '')
            
            # Descriptionã‚’æŠ½å‡º
            description_element = soup.find('div', class_='description') or soup.find('p', class_='world-description')
            if description_element:
                world_info['description'] = description_element.get_text(strip=True)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            meta_elements = soup.find_all('meta')
            for meta in meta_elements:
                property_name = meta.get('property', '').lower()
                name = meta.get('name', '').lower()
                content = meta.get('content', '')
                
                if 'title' in property_name or 'title' in name:
                    if not world_info['title']:
                        world_info['title'] = content
                elif 'description' in property_name or 'description' in name:
                    if not world_info['description']:
                        world_info['description'] = content
                elif 'image' in property_name or 'image' in name:
                    if not world_info['thumbnail_url']:
                        world_info['thumbnail_url'] = content
            
            # JSON-LDã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            json_ld_scripts = soup.find_all('script', type='application/ld+json')
            for script in json_ld_scripts:
                try:
                    data = json.loads(script.string)
                    if isinstance(data, dict):
                        if 'name' in data and not world_info['title']:
                            world_info['title'] = data['name']
                        if 'description' in data and not world_info['description']:
                            world_info['description'] = data['description']
                        if 'image' in data and not world_info['thumbnail_url']:
                            world_info['thumbnail_url'] = data['image']
                        if 'author' in data and not world_info['creator']:
                            author = data['author']
                            if isinstance(author, dict):
                                world_info['creator'] = author.get('name', '')
                            else:
                                world_info['creator'] = str(author)
                except json.JSONDecodeError:
                    continue
            
            # React Propsæˆ–ã„ã¯Next.jsã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            script_elements = soup.find_all('script')
            for script in script_elements:
                if script.string and 'world' in script.string.lower():
                    try:
                        # __NEXT_DATA__ã‚„React propsã‚’æ¢ã™
                        script_text = script.string
                        if '__NEXT_DATA__' in script_text:
                            # Next.jsã®ãƒ‡ãƒ¼ã‚¿ã‚’è§£æ
                            start = script_text.find('{')
                            end = script_text.rfind('}') + 1
                            if start != -1 and end != -1:
                                data = json.loads(script_text[start:end])
                                # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’å†å¸°çš„ã«æ¢ç´¢
                                self._extract_from_json_data(data, world_info)
                    except:
                        continue
            
            # Capacityã¨publishedã®æƒ…å ±ã‚’æ¤œç´¢
            text_content = soup.get_text()
            
            # Capacityã‚’æŠ½å‡º
            capacity_match = re.search(r'capacity[:\s]*(\d+)', text_content, re.I)
            if capacity_match:
                world_info['capacity'] = capacity_match.group(1)
            
            # Publishedæ—¥ä»˜ã‚’æŠ½å‡º
            published_patterns = [
                r'published[:\s]*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'created[:\s]*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})'
            ]
            for pattern in published_patterns:
                match = re.search(pattern, text_content, re.I)
                if match:
                    world_info['published'] = match.group(1)
                    break
            
            return world_info
            
        except Exception as e:
            logger.error(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±æŠ½å‡ºä¾‹å¤–: {e}")
            return None
    
    def _extract_from_json_data(self, data, world_info):
        """
        JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å†å¸°çš„ã«ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã‚’æŠ½å‡º
        """
        if isinstance(data, dict):
            for key, value in data.items():
                key_lower = key.lower()
                if key_lower == 'name' and not world_info['title']:
                    world_info['title'] = str(value)
                elif key_lower == 'description' and not world_info['description']:
                    world_info['description'] = str(value)
                elif key_lower in ['imageurl', 'thumbnailurl', 'image'] and not world_info['thumbnail_url']:
                    world_info['thumbnail_url'] = str(value)
                elif key_lower in ['author', 'creator'] and not world_info['creator']:
                    if isinstance(value, dict):
                        world_info['creator'] = value.get('name', str(value))
                    else:
                        world_info['creator'] = str(value)
                elif key_lower == 'capacity' and not world_info['capacity']:
                    world_info['capacity'] = str(value)
                elif key_lower in ['published', 'createdat', 'publicatedat'] and not world_info['published']:
                    world_info['published'] = str(value)
                elif isinstance(value, (dict, list)):
                    self._extract_from_json_data(value, world_info)
        elif isinstance(data, list):
            for item in data:
                self._extract_from_json_data(item, world_info)

    def scrape_world_by_url(self, url: str) -> Optional[Dict[str, Any]]:
        """
        VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰URLã‹ã‚‰æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        """
        # URLã‹ã‚‰ãƒ¯ãƒ¼ãƒ«ãƒ‰IDã‚’æŠ½å‡º
        world_id_match = re.search(r'wrld_[a-f0-9-]+', url)
        if not world_id_match:
            logger.error(f"ç„¡åŠ¹ãªãƒ¯ãƒ¼ãƒ«ãƒ‰URL: {url}")
            return None
        
        world_id = world_id_match.group(0)
        logger.info(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰IDæŠ½å‡º: {world_id}")
        
        # ã¾ãšVRChat APIã§ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã‚’å–å¾—ã‚’è©¦è¡Œ
        api_data = self.get_world_details(world_id)
        if api_data:
            # APIã‹ã‚‰å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›
            world_info = {
                'world_id': world_id,
                'title': api_data.get('name', ''),
                'creator': api_data.get('authorName', ''),
                'thumbnail_url': api_data.get('imageUrl', ''),
                'description': api_data.get('description', ''),
                'capacity': str(api_data.get('capacity', '')),
                'published': api_data.get('publicatedAt', api_data.get('createdAt', '')),
                'scraped_at': datetime.now().isoformat()
            }
            
            # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            thumbnail_url = api_data.get('imageUrl', '')
            logger.info(f"ã‚µãƒ ãƒã‚¤ãƒ«URLç¢ºèª: '{thumbnail_url}' (é•·ã•: {len(str(thumbnail_url))})")
            if thumbnail_url:
                logger.info(f"ã‚µãƒ ãƒã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {world_id}")
                thumbnail_path = self.download_thumbnail(str(thumbnail_url), world_id)
                if thumbnail_path:
                    world_info['thumbnail_path'] = thumbnail_path
                    logger.info(f"ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š: {thumbnail_path}")
                else:
                    logger.warning(f"ã‚µãƒ ãƒã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {world_id}")
            else:
                logger.warning(f"ã‚µãƒ ãƒã‚¤ãƒ«URLãŒç©ºã¾ãŸã¯None - ãƒ¯ãƒ¼ãƒ«ãƒ‰ID: {world_id}")
            logger.info(f"APIçµŒç”±ã§ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±å–å¾—æˆåŠŸ: {world_info['title']}")
            return world_info
        
        # APIã§ã®å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€Webãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’è©¦è¡Œ
        return self.scrape_world_page(world_id)

    def scrape_worlds_from_file(self, file_path: str) -> List[Dict]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¯ãƒ¼ãƒ«ãƒ‰URLãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ãƒãƒƒãƒå‡¦ç†
        """
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰URLã‚’èª­ã¿è¾¼ã¿
            with open(file_path, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip() and line.strip().startswith('https://vrchat.com/home/world/')]
            
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ {len(urls)} ä»¶ã®VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰URLã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            
            if not urls:
                logger.warning("æœ‰åŠ¹ãªVRChatãƒ¯ãƒ¼ãƒ«ãƒ‰URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return []
            
            return self.scrape_multiple_worlds(urls)
            
        except FileNotFoundError:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return []
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def scrape_multiple_worlds(self, urls: List[str], delay: float = 2.0) -> List[Dict[str, Any]]:
        """
        è¤‡æ•°ã®ãƒ¯ãƒ¼ãƒ«ãƒ‰URLã‚’ãƒãƒƒãƒå‡¦ç†ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        """
        results = []
        total_urls = len(urls)
        
        logger.info(f"ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {total_urls}ä»¶ã®ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¾ã™")
        
        # æœ€åˆã«ä¸€åº¦ã ã‘èªè¨¼ã‚’ç¢ºä¿
        if not self.ensure_authenticated():
            logger.error("èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒãƒƒãƒå‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
            return []
        
        logger.info("èªè¨¼å®Œäº†ã€‚ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        
        for i, url in enumerate(urls, 1):
            try:
                logger.info(f"é€²è¡ŒçŠ¶æ³: {i}/{total_urls} - {url}")
                
                # ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã‚’å–å¾—ï¼ˆèªè¨¼ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼‰
                logger.info(f"_scrape_world_by_url_authenticatedå‘¼ã³å‡ºã—é–‹å§‹: {url}")
                world_info = self._scrape_world_by_url_authenticated(url)  # type: ignore
                logger.info(f"_scrape_world_by_url_authenticatedå®Œäº†: {world_info is not None}")
                
                if world_info:
                    results.append(world_info)
                    logger.info(f"âœ… æˆåŠŸ: {world_info.get('title', 'Unknown')} (ä½œè€…: {world_info.get('creator', 'Unknown')})")  # type: ignore
                    
                    # Firebaseã«ä¿å­˜
                    try:
                        self.firebase_manager.save_vrchat_world_data(world_info)
                        logger.info(f"ğŸ“Š Firebaseä¿å­˜å®Œäº†: {world_info['world_id']}")
                    except Exception as e:
                        logger.error(f"Firebaseä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                else:
                    logger.warning(f"âŒ å¤±æ•—: {url}")
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®ãŸã‚ã®å¾…æ©Ÿï¼ˆæœ€å¾Œã®ã‚¢ã‚¤ãƒ†ãƒ ä»¥å¤–ï¼‰
                if i < total_urls:
                    logger.info(f"â±ï¸  {delay}ç§’å¾…æ©Ÿä¸­...")
                    time.sleep(delay)
                    
            except KeyboardInterrupt:
                logger.info("âš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
                break
            except Exception as e:
                logger.error(f"âŒ URLå‡¦ç†ã‚¨ãƒ©ãƒ¼ {url}: {e}")
                continue
        
        logger.info(f"ğŸ‰ ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(results)}/{total_urls} ä»¶ã®ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")  # type: ignore
        return results
    
    def _scrape_world_by_url_authenticated(self, url: str) -> Optional[Dict[str, Any]]:
        """
        èªè¨¼æ¸ˆã¿ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã‚’å–å¾—ï¼ˆèªè¨¼ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        """
        logger.info(f"_scrape_world_by_url_authenticatedé–‹å§‹: {url}")
        # URLã‹ã‚‰ãƒ¯ãƒ¼ãƒ«ãƒ‰IDã‚’æŠ½å‡º
        world_id_match = re.search(r'wrld_[a-f0-9-]+', url)
        if not world_id_match:
            logger.error(f"ç„¡åŠ¹ãªãƒ¯ãƒ¼ãƒ«ãƒ‰URL: {url}")
            return None
        
        world_id = world_id_match.group(0)
        logger.info(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰IDæŠ½å‡º: {world_id}")
        
        # APIã‹ã‚‰ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã‚’å–å¾—ï¼ˆèªè¨¼ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        api_data = self._get_world_details_authenticated(world_id)
        logger.info(f"APIå–å¾—ãƒ‡ãƒ¼ã‚¿ç¢ºèª: {api_data is not None}")
        if api_data:
            logger.info(f"APIãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼: {list(api_data.keys()) if isinstance(api_data, dict) else 'dictä»¥å¤–'}")
            # APIã‹ã‚‰å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›
            world_info: Dict[str, Any] = {
                'world_id': world_id,
                'title': str(api_data.get('name', '')),  # type: ignore
                'creator': str(api_data.get('authorName', '')),  # type: ignore
                'thumbnail_url': str(api_data.get('imageUrl', '')),  # type: ignore
                'description': str(api_data.get('description', '')),  # type: ignore
                'capacity': str(api_data.get('capacity', '')),  # type: ignore
                'published': str(api_data.get('publicatedAt', api_data.get('createdAt', ''))),  # type: ignore
                'scraped_at': datetime.now().isoformat()
            }
            
            # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            thumbnail_url = str(api_data.get('imageUrl', ''))  # type: ignore
            logger.info(f"ã‚µãƒ ãƒã‚¤ãƒ«URLç¢ºèª: '{thumbnail_url}' (é•·ã•: {len(thumbnail_url)})")
            if thumbnail_url:
                logger.info(f"ã‚µãƒ ãƒã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {world_id}")
                thumbnail_path = self.download_thumbnail(thumbnail_url, world_id)
                if thumbnail_path:
                    world_info['thumbnail_path'] = thumbnail_path
                    logger.info(f"ã‚µãƒ ãƒã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š: {thumbnail_path}")
                else:
                    logger.warning(f"ã‚µãƒ ãƒã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {world_id}")
            else:
                logger.warning(f"ã‚µãƒ ãƒã‚¤ãƒ«URLãŒç©ºã¾ãŸã¯None - ãƒ¯ãƒ¼ãƒ«ãƒ‰ID: {world_id}")
            
            logger.info(f"APIçµŒç”±ã§ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±å–å¾—æˆåŠŸ: {world_info['title']}")
            return world_info
        
        logger.warning(f"APIçµŒç”±ã§ã®ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±å–å¾—ã«å¤±æ•—: {world_id}")
        return None
    
    def _get_world_details_authenticated(self, world_id: str) -> Optional[Dict[str, Any]]:
        """
        èªè¨¼æ¸ˆã¿ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ãƒ¯ãƒ¼ãƒ«ãƒ‰è©³ç´°ã‚’å–å¾—ï¼ˆèªè¨¼ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        """
        try:
            session = self.auth.get_authenticated_session()
            if not session:
                return None
            
            url = urljoin(self.base_url, f"worlds/{world_id}")
            
            logger.info(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰è©³ç´°å–å¾—: {world_id}")
            response = session.get(url)
            
            if response.status_code == 200:
                world_data = response.json()
                logger.info(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰è©³ç´°å–å¾—æˆåŠŸ: {world_data.get('name', 'Unknown')}")
                return world_data
            else:
                logger.error(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰è©³ç´°å–å¾—å¤±æ•—: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            logger.error(f"ãƒ¯ãƒ¼ãƒ«ãƒ‰è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    scraper = VRChatWorldScraper()
    
    try:
        # æŒ‡å®šã•ã‚ŒãŸãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        test_url = "https://vrchat.com/home/world/wrld_1cc734ca-afb8-490f-8aec-2aab3779bcf5"
        world_info = scraper.scrape_world_by_url(test_url)
        
        if world_info:
            print("\n=== VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ± ===")
            print(f"ã‚¿ã‚¤ãƒˆãƒ«: {world_info['title']}")
            print(f"åˆ¶ä½œè€…: {world_info['creator']}")
            print(f"ã‚µãƒ ãƒã‚¤ãƒ«: {world_info['thumbnail_url']}")
            print(f"èª¬æ˜: {world_info['description']}")
            print(f"å®šå“¡: {world_info['capacity']}")
            print(f"å…¬é–‹æ—¥: {world_info['published']}")
            
            # Firebaseã«ä¿å­˜
            scraper.firebase_manager.save_vrchat_world_data(world_info)
        else:
            print("ãƒ¯ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
    except KeyboardInterrupt:
        print("\nå‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        scraper.cleanup()

def old_main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    scraper = VRChatWorldScraper()
    
    try:
        # äººæ°—ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚’20ä»¶å–å¾—
        worlds = scraper.scrape_popular_worlds(count=20)
        print(f"\nå–å¾—ã—ãŸãƒ¯ãƒ¼ãƒ«ãƒ‰æ•°: {len(worlds)}")
        
        for i, world in enumerate(worlds[:5], 1):
            print(f"{i}. {world['name']} (è¨ªå•: {world['visits']}, äººæ°—: {world['popularity']})")
            
    except KeyboardInterrupt:
        print("\nå‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        scraper.cleanup()

if __name__ == "__main__":
    main()
