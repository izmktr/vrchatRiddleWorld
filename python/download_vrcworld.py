#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼

vrcworld.txtã«ã‚ã‚‹ãƒ¯ãƒ¼ãƒ«ãƒ‰URLã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€
ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã¨APIãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã™ã€‚
"""

import os
import sys
import time
from typing import List


# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã§è¿½åŠ 
lib_path = os.path.abspath(os.path.join(os.path.dirname(__file__), 'lib'))
if lib_path not in sys.path:
    sys.path.insert(0, lib_path)

from lib.vrchat_scraper import VRChatWorldScraper
from lib.utils import load_world_urls, save_raw_data


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸŒ VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼")
    print("=" * 50)
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–
    scraper = VRChatWorldScraper()
    
    # ãƒ¯ãƒ¼ãƒ«ãƒ‰URLãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®vrcworld.txtã‚’å‚ç…§ï¼‰
    vrcworld_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'vrcworld.txt')
    world_urls = load_world_urls(vrcworld_path)
    if not world_urls:
        print("âŒ vrcworld.txtã«ãƒ¯ãƒ¼ãƒ«ãƒ‰URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    print(f"ğŸ“‹ {len(world_urls)}ä»¶ã®ãƒ¯ãƒ¼ãƒ«ãƒ‰URLã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    print("-" * 50)
    
    success_count = 0
    skip_count = 0
    error_count = 0
    error_worlds: List[str] = []  # ã‚¨ãƒ©ãƒ¼ãƒ¯ãƒ¼ãƒ«ãƒ‰ã®ãƒªã‚¹ãƒˆ
    
    for i, url in enumerate(world_urls, 1):
        print(f"\nğŸ”„ [{i}/{len(world_urls)}] å‡¦ç†ä¸­: {url}")
        
        try:
            # ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            world_data = scraper.scrape_world_by_url(url)
            if not world_data:
                print(f"âŒ ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—: {url}")
                error_count += 1
                error_worlds.append(f"{url} - ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
                continue
            
            world_id = world_data.get('id')
            if not world_id:
                print(f"âŒ ãƒ¯ãƒ¼ãƒ«ãƒ‰IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {url}")
                error_count += 1
                error_worlds.append(f"{url} - ãƒ¯ãƒ¼ãƒ«ãƒ‰IDä¸æ˜")
                continue
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸã‹ã©ã†ã‹ã‚’åˆ¤å®š
            is_from_cache = world_data.get('_from_cache', False)
            
            # ã‚µãƒ ãƒã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            thumbnail_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'thumbnail')
            thumbnail_result = scraper.download_thumbnail(world_data, thumbnail_dir)
            if thumbnail_result:
                status, _ = thumbnail_result
                if status == 'downloaded':
                    print(f"âœ… ã‚µãƒ ãƒã‚¤ãƒ«: {world_id}.jpgï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼‰")
                elif status == 'skipped':
                    print(f"â­ï¸  ã‚µãƒ ãƒã‚¤ãƒ«: {world_id}.jpgï¼ˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰")
                else:
                    print(f"âš ï¸  ã‚µãƒ ãƒã‚¤ãƒ«: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
            else:
                print(f"âš ï¸  ã‚µãƒ ãƒã‚¤ãƒ«: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
            
            # ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆsave_raw_dataã®ä»•æ§˜ã«åˆã‚ã›ã¦world_dataã‚’ç›´æ¥æ¸¡ã™ï¼‰
            raw_data_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'raw_data')
            if save_raw_data(world_data, raw_data_dir):
                if is_from_cache:
                    print(f"âœ… ç”Ÿãƒ‡ãƒ¼ã‚¿: {world_id}ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰")
                    skip_count += 1
                else:
                    print(f"âœ… ç”Ÿãƒ‡ãƒ¼ã‚¿: {world_id}ï¼ˆä¿å­˜å®Œäº†ï¼‰")
                    success_count += 1
            else:
                print(f"âŒ ç”Ÿãƒ‡ãƒ¼ã‚¿: ä¿å­˜å¤±æ•—")
                error_count += 1
                error_worlds.append(f"{url} - ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜å¤±æ•— (ID: {world_id})")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆ2ç§’é–“éš”ï¼‰- æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
            if i < len(world_urls) and not is_from_cache:
                print("â³ 2ç§’å¾…æ©Ÿä¸­...")
                time.sleep(2)
                
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            error_count += 1
            error_worlds.append(f"{url} - ä¾‹å¤–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            continue
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 50)
    print("ğŸ“Š å‡¦ç†çµæœã‚µãƒãƒªãƒ¼")
    print(f"âœ… æˆåŠŸ: {success_count}ä»¶")
    print(f"â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: {skip_count}ä»¶")
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
    print(f"ğŸ“‹ åˆè¨ˆ: {len(world_urls)}ä»¶")
    print("=" * 50)
    
    # ã‚¨ãƒ©ãƒ¼ãƒ¯ãƒ¼ãƒ«ãƒ‰ã®ãƒ­ã‚°å‡ºåŠ›
    if error_worlds:
        error_log_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'error_world.txt')
        try:
            with open(error_log_path, 'w', encoding='utf-8') as f:
                f.write(f"# ã‚¨ãƒ©ãƒ¼ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ­ã‚° - {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# ç·ã‚¨ãƒ©ãƒ¼æ•°: {error_count}ä»¶\n\n")
                for error_entry in error_worlds:
                    f.write(f"{error_entry}\n")
            print(f"ğŸ“ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’å‡ºåŠ›ã—ã¾ã—ãŸ: error_world.txt ({error_count}ä»¶)")
        except Exception as e:
            print(f"âš ï¸  ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®å‡ºåŠ›ã«å¤±æ•—: {str(e)}")
    else:
        print("ğŸ‰ ã‚¨ãƒ©ãƒ¼ãªãå…¨ã¦å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")


if __name__ == "__main__":
    main()
