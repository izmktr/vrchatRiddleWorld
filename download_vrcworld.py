#!/usr/bin/env python3
"""
VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼
vrcworld.txtã‹ã‚‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¿å­˜
"""

import os
import sys
import time
from typing import List

# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(os.path.join(os.path.dirname(__file__), 'python', 'lib'))

from utils import setup_logging, load_world_urls, save_raw_data, ensure_directory
from vrchat_scraper import VRChatWorldScraper

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    setup_logging()
    
    print("ğŸš€ VRChatãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼é–‹å§‹")
    print("=" * 60)
    
    # è¨­å®š
    world_urls_file = "vrcworld.txt"
    thumbnail_dir = "thumbnail"
    raw_data_dir = "raw_data"
    delay_seconds = 2.0
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    ensure_directory(thumbnail_dir)
    ensure_directory(raw_data_dir)
    
    # URLãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿
    urls = load_world_urls(world_urls_file)
    if not urls:
        print("âŒ å‡¦ç†å¯èƒ½ãªURLãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    print(f"ğŸ“‹ å‡¦ç†å¯¾è±¡: {len(urls)} ä»¶ã®ãƒ¯ãƒ¼ãƒ«ãƒ‰")
    print(f"ğŸ“ ã‚µãƒ ãƒã‚¤ãƒ«ä¿å­˜å…ˆ: {thumbnail_dir}")
    print(f"ğŸ“ ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ: {raw_data_dir}")
    print(f"â±ï¸ å‡¦ç†é–“éš”: {delay_seconds} ç§’")
    print()
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–
    scraper = VRChatWorldScraper()
    
    try:
        successful_count = 0
        failed_count = 0
        
        for i, url in enumerate(urls, 1):
            print(f"ğŸ“ é€²è¡ŒçŠ¶æ³: {i}/{len(urls)} ({i/len(urls)*100:.1f}%)")
            print(f"ğŸ”— å‡¦ç†ä¸­: {url}")
            
            try:
                # ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿å–å¾—
                world_data = scraper.scrape_world_by_url(url)
                
                if world_data:
                    world_id = world_data.get('id', 'unknown')
                    world_name = world_data.get('name', 'Unknown')
                    
                    print(f"âœ… å–å¾—æˆåŠŸ: {world_name} ({world_id})")
                    
                    # ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆä¸Šæ›¸ãï¼‰
                    raw_file = save_raw_data(world_data, raw_data_dir)
                    if raw_file:
                        print(f"ğŸ’¾ ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜: {os.path.basename(raw_file)}")
                    
                    # ã‚µãƒ ãƒã‚¤ãƒ«ä¿å­˜ï¼ˆã‚¹ã‚­ãƒƒãƒ—ã‚ã‚Šï¼‰
                    thumbnail_file = scraper.download_thumbnail(world_data, thumbnail_dir)
                    if thumbnail_file:
                        if os.path.basename(thumbnail_file) in [f for f in os.listdir(thumbnail_dir) if f.endswith('.jpg')]:
                            action = "ã‚¹ã‚­ãƒƒãƒ—" if "ã‚¹ã‚­ãƒƒãƒ—" in str(thumbnail_file) else "ä¿å­˜"
                            print(f"ğŸ“· ã‚µãƒ ãƒã‚¤ãƒ«{action}: {os.path.basename(thumbnail_file)}")
                    
                    successful_count += 1
                else:
                    print(f"âŒ å–å¾—å¤±æ•—: {url}")
                    failed_count += 1
                
                # é€²è¡ŒçŠ¶æ³è¡¨ç¤º
                if i < len(urls):
                    print(f"ğŸ’¤ {delay_seconds}ç§’å¾…æ©Ÿä¸­...")
                    print()
                    time.sleep(delay_seconds)
                
            except KeyboardInterrupt:
                print("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
                break
            except Exception as e:
                print(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                failed_count += 1
                continue
        
        # çµæœã‚µãƒãƒªãƒ¼
        print("=" * 60)
        print("ğŸ‰ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†å®Œäº†!")
        print("=" * 60)
        print(f"âœ… æˆåŠŸ: {successful_count} ä»¶")
        print(f"âŒ å¤±æ•—: {failed_count} ä»¶")
        if successful_count + failed_count > 0:
            success_rate = successful_count / (successful_count + failed_count) * 100
            print(f"ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ
        thumbnail_files = [f for f in os.listdir(thumbnail_dir) if f.endswith('.jpg')]
        raw_files = [f for f in os.listdir(raw_data_dir) if f.startswith('vrchat_raw_')]
        
        print(f"\nğŸ“ˆ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ:")
        print(f"ãƒ»ã‚µãƒ ãƒã‚¤ãƒ«: {len(thumbnail_files)} ä»¶")
        print(f"ãƒ»ç”Ÿãƒ‡ãƒ¼ã‚¿: {len(raw_files)} ä»¶")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        scraper.cleanup()

if __name__ == "__main__":
    main()
